#!/usr/bin/env python3
"""
Device Troubleshooting Script for Ki-67 Champion Training

This script helps identify and fix device mismatch issues in the champion training.
"""

import torch
import torch.nn as nn
import numpy as np

def test_device_compatibility():
    """Test device compatibility and identify issues"""
    print("ğŸ” Device Compatibility Test")
    print("=" * 50)
    
    # Check CUDA availability
    print(f"ğŸš€ CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ğŸš€ GPU Count: {torch.cuda.device_count()}")
        print(f"ğŸš€ Current Device: {torch.cuda.current_device()}")
        print(f"ğŸš€ Device Name: {torch.cuda.get_device_name(0)}")
        print(f"ğŸš€ Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ”§ Using device: {device}")
    
    # Test basic tensor operations
    print("\nğŸ“Š Testing Basic Tensor Operations:")
    try:
        x = torch.randn(2, 3, 224, 224).to(device)
        y = torch.randn(2, 1).to(device)
        print(f"âœ… Created tensors on {device}")
        print(f"   x.device: {x.device}")
        print(f"   y.device: {y.device}")
    except Exception as e:
        print(f"âŒ Failed to create tensors: {e}")
        return False
    
    # Test model creation
    print("\nğŸ—ï¸ Testing Model Creation:")
    try:
        # Simple test model
        model = nn.Sequential(
            nn.Conv2d(3, 16, 3),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(16, 1)
        ).to(device)
        
        print(f"âœ… Created model on {device}")
        print(f"   Model device: {next(model.parameters()).device}")
        
        # Test forward pass
        with torch.no_grad():
            output = model(x)
            print(f"âœ… Forward pass successful")
            print(f"   Output device: {output.device}")
            print(f"   Output shape: {output.shape}")
            
    except Exception as e:
        print(f"âŒ Model test failed: {e}")
        return False
    
    # Test mixup operations
    print("\nğŸ”„ Testing Mixup Operations:")
    try:
        from train_efficientnet_champion import mixup_data, cutmix_data
        
        # Test mixup
        mixed_x, y_a, y_b, lam = mixup_data(x, y, alpha=0.3)
        print(f"âœ… Mixup successful")
        print(f"   mixed_x.device: {mixed_x.device}")
        print(f"   y_a.device: {y_a.device}")
        print(f"   y_b.device: {y_b.device}")
        
        # Test cutmix
        cut_x, cut_y_a, cut_y_b, cut_lam = cutmix_data(x.clone(), y, alpha=0.8)
        print(f"âœ… CutMix successful")
        print(f"   cut_x.device: {cut_x.device}")
        print(f"   cut_y_a.device: {cut_y_a.device}")
        print(f"   cut_y_b.device: {cut_y_b.device}")
        
    except Exception as e:
        print(f"âŒ Augmentation test failed: {e}")
        return False
    
    # Test mixed precision
    print("\nâš¡ Testing Mixed Precision:")
    try:
        scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None
        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
        criterion = nn.BCEWithLogitsLoss()
        
        model.train()
        optimizer.zero_grad()
        
        if scaler is not None:
            with torch.cuda.amp.autocast():
                output = model(x)
                if output.dim() == 1:
                    output = output.unsqueeze(1)
                loss = criterion(output, y)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            print(f"âœ… Mixed precision training successful")
        else:
            output = model(x)
            if output.dim() == 1:
                output = output.unsqueeze(1)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
            print(f"âœ… Regular precision training successful")
            
        print(f"   Loss: {loss.item():.4f}")
        
    except Exception as e:
        print(f"âŒ Training test failed: {e}")
        return False
    
    print(f"\nğŸ‰ All device compatibility tests passed!")
    return True

def diagnose_device_issues():
    """Diagnose common device issues"""
    print("\nğŸ”§ Device Issue Diagnosis:")
    print("-" * 30)
    
    issues_found = []
    
    # Check for CUDA initialization
    try:
        torch.cuda.init()
        print("âœ… CUDA initialized successfully")
    except Exception as e:
        issues_found.append(f"CUDA initialization failed: {e}")
    
    # Check memory
    if torch.cuda.is_available():
        try:
            allocated = torch.cuda.memory_allocated(0)
            reserved = torch.cuda.memory_reserved(0)
            print(f"âœ… GPU Memory - Allocated: {allocated/1024**2:.1f}MB, Reserved: {reserved/1024**2:.1f}MB")
        except Exception as e:
            issues_found.append(f"Memory check failed: {e}")
    
    # Check tensor creation consistency
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        for i in range(5):
            t = torch.randn(10, 10, device=device)
            assert t.device == device, f"Tensor {i} on wrong device: {t.device}"
        print("âœ… Consistent tensor creation")
    except Exception as e:
        issues_found.append(f"Inconsistent tensor creation: {e}")
    
    if issues_found:
        print(f"\nâš ï¸  Issues found:")
        for issue in issues_found:
            print(f"   â€¢ {issue}")
    else:
        print(f"\nâœ… No device issues detected")
    
    return len(issues_found) == 0

if __name__ == "__main__":
    print("ğŸ† Ki-67 Champion Device Troubleshooting")
    print("=" * 60)
    
    # Run compatibility tests
    compatibility_ok = test_device_compatibility()
    
    # Run diagnostics
    diagnostics_ok = diagnose_device_issues()
    
    print(f"\nğŸ“‹ Summary:")
    print(f"   Compatibility Test: {'âœ… PASS' if compatibility_ok else 'âŒ FAIL'}")
    print(f"   Diagnostics: {'âœ… PASS' if diagnostics_ok else 'âŒ FAIL'}")
    
    if compatibility_ok and diagnostics_ok:
        print(f"\nğŸ‰ Your system is ready for champion training!")
        print(f"ğŸ’¡ The device mismatch issue may be in the dataset loading or specific model components.")
    else:
        print(f"\nâš ï¸  Device issues detected. Please fix before training.")
