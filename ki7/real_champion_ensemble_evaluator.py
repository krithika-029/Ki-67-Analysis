#!/usr/bin/env python3
"""
Real Champion Ensemble Evaluator - 95%+ Target
Uses actual trained models from Google Drive for maximum performance

Models Available:
- EfficientNet-B5 Champion (90.98% validation) - STAR MODEL
- EfficientNet-B4 Adapted Champion 
- EfficientNet-B4 T4 Champion
- EfficientNet-B2 Advanced
- ConvNeXt-Tiny Advanced
- DenseNet-121 Advanced
- Vision Transformer (ViT)
- Swin Transformer Tiny
- ResNet50, InceptionV3, RegNet-Y-8GF

TARGET: 95%+ accuracy using real trained models
"""

import os
import sys
import warnings
import json
import numpy as np
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# Suppress warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from PIL import Image

try:
    import timm
    print(f"âœ… timm version: {timm.__version__}")
except ImportError:
    print("Installing timm...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "timm"])
    import timm

from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix, 
    roc_auc_score, roc_curve, precision_score, recall_score, f1_score
)

class RealKi67Dataset(Dataset):
    """Dataset for real model ensemble evaluation"""
    
    def __init__(self, dataset_path, split='test', transform=None, use_tta=False, tta_variants=5):
        self.dataset_path = Path(dataset_path)
        self.split = split
        self.transform = transform
        self.use_tta = use_tta
        self.tta_variants = tta_variants
        
        self.images = []
        self.labels = []
        
        self.create_dataset_from_directory()
    
    def create_dataset_from_directory(self):
        """Create dataset using proven ensemble pipeline logic"""
        print(f"ğŸ”§ Creating {self.split} dataset from directory structure...")
        
        # Try different possible dataset structures
        possible_paths = [
            self.dataset_path / "Ki67_Dataset_for_Colab",
            self.dataset_path / "BCData",
            self.dataset_path / "ki67_dataset",
            self.dataset_path
        ]
        
        base_path = None
        for path in possible_paths:
            if path.exists():
                if (path / "images" / self.split).exists() and (path / "annotations" / self.split).exists():
                    base_path = path
                    break
        
        if base_path is None:
            print(f"âŒ No valid dataset path found")
            return
        
        images_dir = base_path / "images" / self.split
        pos_annotations_dir = base_path / "annotations" / self.split / "positive"
        neg_annotations_dir = base_path / "annotations" / self.split / "negative"
        
        if not images_dir.exists():
            print(f"âŒ Images directory not found: {images_dir}")
            return
        
        print(f"ğŸ“ Loading from: {images_dir}")
        
        # Use proven annotation file size logic
        for img_file in images_dir.glob("*.png"):
            img_name = img_file.stem
            
            pos_ann = pos_annotations_dir / f"{img_name}.h5"
            neg_ann = neg_annotations_dir / f"{img_name}.h5"
            
            if pos_ann.exists() and neg_ann.exists():
                try:
                    pos_size = pos_ann.stat().st_size
                    neg_size = neg_ann.stat().st_size
                    size_diff = abs(pos_size - neg_size)
                    
                    if size_diff > 100:
                        if neg_size > pos_size:
                            self.images.append(str(img_file))
                            self.labels.append(0)  # Negative
                        else:
                            self.images.append(str(img_file))
                            self.labels.append(1)  # Positive
                    else:
                        idx = len(self.images)
                        self.images.append(str(img_file))
                        self.labels.append(idx % 2)
                        
                except Exception as e:
                    idx = len(self.images)
                    self.images.append(str(img_file))
                    self.labels.append(idx % 2)
        
        pos_count = sum(self.labels)
        neg_count = len(self.labels) - pos_count
        print(f"âœ… Found {len(self.images)} images")
        print(f"   Distribution: {pos_count} positive, {neg_count} negative")
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        img_path = self.images[idx]
        label = self.labels[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
            
            if self.use_tta:
                # Return multiple augmented versions for TTA
                images = []
                for _ in range(self.tta_variants):
                    if self.transform:
                        aug_img = self.transform(image)
                    else:
                        aug_img = transforms.ToTensor()(image)
                    images.append(aug_img)
                return images, torch.tensor(label, dtype=torch.float32)
            else:
                if self.transform:
                    image = self.transform(image)
                else:
                    image = transforms.ToTensor()(image)
                return image, torch.tensor(label, dtype=torch.float32)
                
        except Exception as e:
            print(f"âš ï¸  Error loading image {img_path}: {e}")
            if self.transform:
                fallback = self.transform(Image.new('RGB', (320, 320), color='black'))
            else:
                fallback = torch.zeros((3, 320, 320), dtype=torch.float32)
            return fallback, torch.tensor(label, dtype=torch.float32)

def create_champion_transforms():
    """Create transforms optimized for champion models"""
    print("ğŸ–¼ï¸ Creating champion transforms...")
    
    # Standard transforms (320x320 for B5 Champion compatibility)
    transform = transforms.Compose([
        transforms.Resize((320, 320)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # TTA transforms with moderate augmentation
    tta_transform = transforms.Compose([
        transforms.Resize((320, 320)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomRotation(degrees=10),
        transforms.ColorJitter(brightness=0.1, contrast=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    print("âœ… Champion transforms created")
    return transform, tta_transform

def load_real_champion_models(device, models_dir):
    """Load the real trained models for champion ensemble"""
    print("ğŸ† Loading REAL trained champion models...")
    
    models = []
    model_info = []
    
    # Champion model configurations (prioritize best performers)
    model_configs = [
        {
            'file': 'Ki67_STABLE_Champion_EfficientNet_B5_Champion_FINAL_90.98_20250620_142507.pth',
            'architecture': 'efficientnet_b5',
            'name': 'EfficientNet-B5-Champion-90.98%',
            'priority': 1  # Highest priority - star model
        },
        {
            'file': 'Ki67_B4_Adapted_Champion_EfficientNet_B4_Adapted_best_model_20250620_133200.pth',
            'architecture': 'efficientnet_b4',
            'name': 'EfficientNet-B4-Adapted-Champion',
            'priority': 2
        },
        {
            'file': 'Ki67_T4_Champion_EfficientNet_B4_best_model_20250620_111518.pth',
            'architecture': 'efficientnet_b4',
            'name': 'EfficientNet-B4-T4-Champion',
            'priority': 3
        },
        {
            'file': 'Ki67_Advanced_EfficientNet-B2_best_model_20250619_105754.pth',
            'architecture': 'efficientnet_b2',
            'name': 'EfficientNet-B2-Advanced',
            'priority': 4
        },
        {
            'file': 'Ki67_Advanced_ConvNeXt-Tiny_best_model_20250619_110232.pth',
            'architecture': 'convnext_tiny',
            'name': 'ConvNeXt-Tiny-Advanced',
            'priority': 5
        },
        {
            'file': 'Ki67_ViT_best_model_20250619_071454.pth',
            'architecture': 'vit_base_patch16_224',
            'name': 'Vision-Transformer',
            'priority': 6
        }
    ]
    
    # Sort by priority (star model first)
    model_configs.sort(key=lambda x: x['priority'])
    
    loaded_count = 0
    max_models = 5  # Limit to best 5 models for optimal ensemble
    
    for config in model_configs:
        if loaded_count >= max_models:
            break
            
        model_path = models_dir / config['file']
        if model_path.exists():
            try:
                print(f"ğŸ“¦ Loading {config['name']}...")
                
                # Create model architecture
                if config['architecture'] == 'vit_base_patch16_224':
                    model = timm.create_model(config['architecture'], pretrained=False, num_classes=1, img_size=320)
                else:
                    model = timm.create_model(config['architecture'], pretrained=False, num_classes=1)
                
                # Load trained weights
                checkpoint = torch.load(model_path, map_location=device)
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                    if 'best_val_acc' in checkpoint:
                        val_acc = checkpoint['best_val_acc']
                        print(f"   Validation accuracy: {val_acc:.2f}%")
                else:
                    model.load_state_dict(checkpoint)
                
                model = model.to(device)
                model.eval()
                
                models.append(model)
                model_info.append(config['name'])
                loaded_count += 1
                print(f"âœ… Loaded {config['name']}")
                
            except Exception as e:
                print(f"âš ï¸  Failed to load {config['name']}: {e}")
                continue
    
    if loaded_count == 0:
        print("âŒ No models loaded successfully")
        return [], []
    
    print(f"\nğŸ¯ Champion Ensemble Configuration:")
    print(f"   Total models loaded: {loaded_count}")
    for i, name in enumerate(model_info):
        print(f"   {i+1}. {name}")
    
    return models, model_info

def ensemble_predict_with_weights(models, data_loader, device, use_tta=True, model_weights=None):
    """Make weighted ensemble predictions with TTA"""
    print(f"ğŸ”® Making weighted ensemble predictions (TTA: {'Enabled' if use_tta else 'Disabled'})...")
    
    if model_weights is None:
        # Default weights: give highest weight to B5 Champion
        model_weights = [0.4, 0.2, 0.2, 0.1, 0.1][:len(models)]
        # Normalize weights
        total_weight = sum(model_weights)
        model_weights = [w/total_weight for w in model_weights]
    
    print(f"ğŸ“Š Model weights: {[f'{w:.2f}' for w in model_weights]}")
    
    all_predictions = []
    all_targets = []
    all_probabilities = []
    
    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(data_loader):
            if batch_idx % 20 == 0:
                print(f"Processing batch {batch_idx+1}/{len(data_loader)}")
            
            targets = targets.to(device)
            
            if use_tta and isinstance(inputs, list):
                # TTA: inputs is a list of augmented versions
                batch_predictions = []
                
                for tta_idx in range(len(inputs)):
                    tta_input = inputs[tta_idx].to(device)
                    
                    # Get weighted predictions from all models for this TTA variant
                    weighted_output = torch.zeros(tta_input.size(0), 1, device=device)
                    
                    for model_idx, model in enumerate(models):
                        try:
                            output = model(tta_input)
                            if output.dim() == 1:
                                output = output.unsqueeze(1)
                            weighted_output += model_weights[model_idx] * torch.sigmoid(output)
                        except Exception as e:
                            print(f"âš ï¸  Error with model {model_idx}: {e}")
                            continue
                    
                    batch_predictions.append(weighted_output)
                
                # Average across TTA variants
                final_predictions = torch.stack(batch_predictions).mean(dim=0)
                
            else:
                # Standard prediction without TTA
                if isinstance(inputs, list):
                    inputs = inputs[0]  # Take first variant if TTA data
                inputs = inputs.to(device)
                
                # Get weighted predictions from all models
                weighted_output = torch.zeros(inputs.size(0), 1, device=device)
                
                for model_idx, model in enumerate(models):
                    try:
                        output = model(inputs)
                        if output.dim() == 1:
                            output = output.unsqueeze(1)
                        weighted_output += model_weights[model_idx] * torch.sigmoid(output)
                    except Exception as e:
                        print(f"âš ï¸  Error with model {model_idx}: {e}")
                        continue
                
                final_predictions = weighted_output
            
            # Convert to binary predictions
            binary_predictions = (final_predictions > 0.5).float()
            
            all_predictions.extend(binary_predictions.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
            all_probabilities.extend(final_predictions.cpu().numpy())
    
    return np.array(all_predictions), np.array(all_targets), np.array(all_probabilities)

def calculate_comprehensive_metrics(predictions, targets, probabilities):
    """Calculate comprehensive evaluation metrics"""
    accuracy = accuracy_score(targets, predictions)
    precision = precision_score(targets, predictions, zero_division=0)
    recall = recall_score(targets, predictions, zero_division=0)
    f1 = f1_score(targets, predictions, zero_division=0)
    
    try:
        auc = roc_auc_score(targets, probabilities)
    except:
        auc = 0.5
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'auc': auc
    }

def save_champion_results(results, save_path):
    """Save champion ensemble results"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = save_path / f"real_champion_ensemble_results_{timestamp}.json"
    
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"ğŸ’¾ Champion results saved to: {results_file}")
    return results_file

def create_champion_visualization(results, save_path):
    """Create comprehensive performance visualization"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    # Accuracy comparison with 95% target
    methods = ['Standard Ensemble', 'TTA Ensemble'] 
    accuracies = [results['standard']['accuracy'] * 100, results['tta']['accuracy'] * 100]
    
    colors = ['lightcoral' if acc < 95 else 'lightgreen' for acc in accuracies]
    bars = ax1.bar(methods, accuracies, color=colors)
    ax1.set_ylabel('Accuracy (%)')
    ax1.set_title('Real Champion Ensemble Performance')
    ax1.set_ylim([80, 100])
    
    # Add 95% target line
    ax1.axhline(y=95, color='red', linestyle='--', linewidth=2, label='95% Target')
    ax1.legend()
    
    # Add accuracy labels on bars
    for bar, acc in zip(bars, accuracies):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')
    
    # Confusion matrix for best method
    best_method = 'tta' if results['tta']['accuracy'] > results['standard']['accuracy'] else 'standard'
    cm = results[best_method]['confusion_matrix']
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2)
    ax2.set_title(f'Best Method Confusion Matrix ({best_method.upper()})')
    ax2.set_xlabel('Predicted')
    ax2.set_ylabel('Actual')
    
    # ROC curve
    fpr, tpr, _ = roc_curve(results[best_method]['targets'], results[best_method]['probabilities'])
    ax3.plot(fpr, tpr, label=f"AUC = {results[best_method]['auc']:.3f}", linewidth=2)
    ax3.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    ax3.set_xlabel('False Positive Rate')
    ax3.set_ylabel('True Positive Rate')
    ax3.set_title(f'ROC Curve - {best_method.upper()} Ensemble')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Comprehensive metrics comparison
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
    standard_values = [
        results['standard']['accuracy'],
        results['standard']['precision'], 
        results['standard']['recall'],
        results['standard']['f1_score'],
        results['standard']['auc']
    ]
    tta_values = [
        results['tta']['accuracy'],
        results['tta']['precision'],
        results['tta']['recall'], 
        results['tta']['f1_score'],
        results['tta']['auc']
    ]
    
    x = np.arange(len(metrics))
    width = 0.35
    
    ax4.bar(x - width/2, standard_values, width, label='Standard', color='skyblue', alpha=0.8)
    ax4.bar(x + width/2, tta_values, width, label='TTA', color='orange', alpha=0.8)
    ax4.set_ylabel('Score')
    ax4.set_title('Comprehensive Metrics Comparison')
    ax4.set_xticks(x)
    ax4.set_xticklabels(metrics)
    ax4.legend()
    ax4.set_ylim([0, 1])
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    viz_file = save_path / f"real_champion_ensemble_performance_{timestamp}.png"
    plt.savefig(viz_file, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“Š Champion visualization saved to: {viz_file}")
    return viz_file

def main():
    """Main function for Real Champion ensemble evaluation"""
    print("ğŸ† Ki-67 REAL CHAMPION Ensemble Evaluator")
    print("=" * 70)
    print("Using ACTUAL trained models from Google Drive")
    print("ğŸ¯ TARGET: 95%+ accuracy with EfficientNet-B5 Champion + Best models")
    print()
    
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ Using device: {device}")
    
    # Paths
    current_dir = Path.cwd()
    models_dir = current_dir / "models"
    results_dir = current_dir / "results"
    results_dir.mkdir(exist_ok=True)
    
    print(f"ğŸ“‚ Models directory: {models_dir}")
    print(f"ğŸ’¾ Results directory: {results_dir}")
    
    # Check models directory
    if not models_dir.exists():
        print("âŒ Models directory not found")
        return
    
    # Try to find dataset
    dataset_paths = ["./Ki67_Dataset_for_Colab", "./BCData", "./data"]
    dataset_path = None
    for path in dataset_paths:
        if Path(path).exists():
            dataset_path = Path(path)
            break
    
    if dataset_path is None:
        print("âŒ Dataset not found. Please ensure dataset is available.")
        return
    
    print(f"ğŸ“Š Dataset: {dataset_path}")
    
    # Create transforms
    standard_transform, tta_transform = create_champion_transforms()
    
    # Load real trained models
    champion_models, model_names = load_real_champion_models(device, models_dir)
    
    if len(champion_models) == 0:
        print("âŒ No models loaded successfully")
        return
    
    # Create datasets
    print(f"\nğŸ“Š Creating evaluation datasets...")
    
    # Standard evaluation dataset
    standard_dataset = RealKi67Dataset(dataset_path, split='test', transform=standard_transform, use_tta=False)
    standard_loader = DataLoader(standard_dataset, batch_size=16, shuffle=False, num_workers=0)
    
    # TTA evaluation dataset  
    tta_dataset = RealKi67Dataset(dataset_path, split='test', transform=tta_transform, use_tta=True, tta_variants=8)
    tta_loader = DataLoader(tta_dataset, batch_size=8, shuffle=False, num_workers=0)
    
    print(f"âœ… Test dataset: {len(standard_dataset)} samples")
    
    # Evaluate Champion ensemble
    print(f"\nğŸ§ª Evaluating REAL Champion Ensemble...")
    
    # Standard ensemble evaluation
    print("ğŸ“ˆ Standard weighted ensemble evaluation...")
    standard_preds, standard_targets, standard_probs = ensemble_predict_with_weights(
        champion_models, standard_loader, device, use_tta=False
    )
    standard_metrics = calculate_comprehensive_metrics(standard_preds, standard_targets, standard_probs)
    
    # TTA ensemble evaluation
    print("ğŸ”„ TTA weighted ensemble evaluation...")
    tta_preds, tta_targets, tta_probs = ensemble_predict_with_weights(
        champion_models, tta_loader, device, use_tta=True
    )
    tta_metrics = calculate_comprehensive_metrics(tta_preds, tta_targets, tta_probs)
    
    # Compile results
    results = {
        'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
        'ensemble_config': {
            'models': model_names,
            'total_models': len(champion_models),
            'device': str(device),
            'star_model': 'EfficientNet-B5-Champion-90.98%'
        },
        'standard': {
            **standard_metrics,
            'confusion_matrix': confusion_matrix(standard_targets, standard_preds).tolist(),
            'targets': standard_targets.tolist(),
            'predictions': standard_preds.tolist(),
            'probabilities': standard_probs.tolist()
        },
        'tta': {
            **tta_metrics,
            'confusion_matrix': confusion_matrix(tta_targets, tta_preds).tolist(),
            'targets': tta_targets.tolist(),
            'predictions': tta_preds.tolist(),
            'probabilities': tta_probs.tolist()
        }
    }
    
    # Display results
    print(f"\nğŸ† REAL CHAMPION ENSEMBLE RESULTS:")
    print(f"=" * 60)
    print(f"ğŸŒŸ Star Model: EfficientNet-B5 Champion (90.98% validation)")
    print(f"ğŸ“¦ Total Models: {len(champion_models)}")
    
    print(f"\nğŸ“Š Standard Weighted Ensemble:")
    print(f"   Accuracy: {standard_metrics['accuracy']*100:.2f}%")
    print(f"   Precision: {standard_metrics['precision']:.3f}")
    print(f"   Recall: {standard_metrics['recall']:.3f}")
    print(f"   F1-Score: {standard_metrics['f1_score']:.3f}")
    print(f"   AUC: {standard_metrics['auc']:.3f}")
    
    print(f"\nğŸ”„ TTA Weighted Ensemble:")
    print(f"   Accuracy: {tta_metrics['accuracy']*100:.2f}%")
    print(f"   Precision: {tta_metrics['precision']:.3f}")
    print(f"   Recall: {tta_metrics['recall']:.3f}")
    print(f"   F1-Score: {tta_metrics['f1_score']:.3f}")
    print(f"   AUC: {tta_metrics['auc']:.3f}")
    
    # Check if 95%+ achieved
    best_accuracy = max(standard_metrics['accuracy'], tta_metrics['accuracy']) * 100
    best_method = 'TTA' if tta_metrics['accuracy'] > standard_metrics['accuracy'] else 'Standard'
    
    print(f"\nğŸ¯ BEST PERFORMANCE: {best_accuracy:.2f}% ({best_method})")
    
    if best_accuracy >= 95.0:
        print(f"\nğŸ‰ğŸ‰ğŸ‰ 95%+ TARGET ACHIEVED! ğŸ‰ğŸ‰ğŸ‰")
        print(f"ğŸ† MISSION ACCOMPLISHED!")
        print(f"ğŸš€ Real Champion Ensemble SUCCESS!")
        print(f"â­ EfficientNet-B5 Champion + ensemble delivers 95%+!")
    elif best_accuracy >= 93.0:
        print(f"\nğŸ”¥ğŸ”¥ EXCEPTIONAL PERFORMANCE! ğŸ”¥ğŸ”¥")
        print(f"ğŸ† {best_accuracy:.2f}% - Very close to 95% target!")
        print(f"ğŸ’¡ Just {(95.0 - best_accuracy):.1f}% away from target")
    elif best_accuracy >= 90.0:
        print(f"\nâœ… STRONG PERFORMANCE!")
        print(f"ğŸ† {best_accuracy:.2f}% - Good ensemble results")
        print(f"ğŸ“ˆ {(95.0 - best_accuracy):.1f}% more needed for 95% target")
    else:
        print(f"\nğŸ“ˆ Current performance: {best_accuracy:.2f}%")
        print(f"ğŸ¯ Need {(95.0 - best_accuracy):.1f}% more for target")
    
    # Save results and create visualization
    results_file = save_champion_results(results, results_dir)
    viz_file = create_champion_visualization(results, results_dir)
    
    print(f"\nğŸ’¡ Recommendations:")
    if best_accuracy >= 95.0:
        print(f"   âœ… TARGET ACHIEVED! Deploy this ensemble in production")
        print(f"   ğŸ† Configuration: {best_method} ensemble with {len(champion_models)} models")
        print(f"   â­ Star performer: EfficientNet-B5 Champion (90.98%)")
    elif best_accuracy >= 93.0:
        print(f"   ğŸ”¥ Very close! Try these optimizations:")
        print(f"   1. Increase TTA variants (currently 8)")
        print(f"   2. Fine-tune ensemble weights")
        print(f"   3. Add confidence thresholding")
    else:
        print(f"   ğŸ“ˆ To reach 95%:")
        print(f"   1. Train additional diverse models")
        print(f"   2. Use advanced ensemble techniques")
        print(f"   3. Apply model calibration")
    
    print(f"\nğŸ“‚ Files created:")
    print(f"   ğŸ“Š Results: {results_file}")
    print(f"   ğŸ“ˆ Visualization: {viz_file}")
    print(f"\nğŸ† Real Champion Ensemble evaluation complete!")
    
    return results

if __name__ == "__main__":
    main()
