#!/usr/bin/env python3
"""
Backend Validation Test Script
Compares backend API predictions with validation script ground truth
"""

import requests
import json
import os
from pathlib import Path
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix

def get_ground_truth_label(image_id):
    """Get ground truth label using annotation file size logic"""
    pos_file = f"Ki67_Dataset_for_Colab/annotations/test/positive/{image_id}.h5"
    neg_file = f"Ki67_Dataset_for_Colab/annotations/test/negative/{image_id}.h5"
    
    if os.path.exists(pos_file) and os.path.exists(neg_file):
        pos_size = os.path.getsize(pos_file)
        neg_size = os.path.getsize(neg_file)
        size_diff = abs(pos_size - neg_size)
        
        if size_diff > 100:  # Significant size difference
            return 1 if pos_size > neg_size else 0
        else:
            # Very similar sizes, use alternating pattern
            return int(image_id) % 2
    elif os.path.exists(pos_file):
        return 1
    elif os.path.exists(neg_file):
        return 0
    else:
        return None

def test_backend_prediction(image_id):
    """Test a single image prediction via backend API"""
    try:
        url = "http://localhost:5002/api/predict"
        image_path = f"Ki67_Dataset_for_Colab/images/test/{image_id}.png"
        
        if not os.path.exists(image_path):
            return None
        
        with open(image_path, 'rb') as f:
            files = {'image': f}
            response = requests.post(url, files=files)
        
        if response.status_code == 200:
            result = response.json()
            return {
                'prediction': result['prediction'],
                'confidence': result['confidence'],
                'high_confidence': result['high_confidence'],
                'probability': result['probability']
            }
        else:
            return None
    except Exception as e:
        print(f"Error testing image {image_id}: {e}")
        return None

def main():
    print("üß™ Backend Prediction Validation Test")
    print("=" * 60)
    
    # Test a sample of images
    test_images = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]
    
    results = []
    correct_predictions = 0
    high_conf_correct = 0
    high_conf_total = 0
    
    print("Testing backend predictions vs ground truth...")
    print("Image | Ground Truth | Prediction | Confidence | High-Conf | Correct")
    print("-" * 70)
    
    for img_id in test_images:
        ground_truth = get_ground_truth_label(str(img_id))
        if ground_truth is None:
            continue
            
        backend_result = test_backend_prediction(str(img_id))
        if backend_result is None:
            continue
        
        prediction = backend_result['prediction']
        confidence = backend_result['confidence']
        high_confidence = backend_result['high_confidence']
        
        is_correct = (prediction == ground_truth)
        if is_correct:
            correct_predictions += 1
        
        if high_confidence:
            high_conf_total += 1
            if is_correct:
                high_conf_correct += 1
        
        gt_label = "Pos" if ground_truth == 1 else "Neg"
        pred_label = "Pos" if prediction == 1 else "Neg"
        conf_str = f"{confidence:.3f}"
        hc_str = "YES" if high_confidence else "NO"
        correct_str = "‚úÖ" if is_correct else "‚ùå"
        
        print(f"{img_id:5d} | {gt_label:11s} | {pred_label:10s} | {conf_str:10s} | {hc_str:9s} | {correct_str}")
        
        results.append({
            'image_id': img_id,
            'ground_truth': ground_truth,
            'prediction': prediction,
            'confidence': confidence,
            'high_confidence': high_confidence,
            'correct': is_correct
        })
    
    # Calculate metrics
    total_tested = len(results)
    overall_accuracy = correct_predictions / total_tested if total_tested > 0 else 0
    high_conf_accuracy = high_conf_correct / high_conf_total if high_conf_total > 0 else 0
    high_conf_coverage = high_conf_total / total_tested if total_tested > 0 else 0
    
    print("\n" + "=" * 60)
    print("üìä BACKEND VALIDATION RESULTS:")
    print(f"   Total images tested: {total_tested}")
    print(f"   Overall accuracy: {overall_accuracy:.3f} ({overall_accuracy*100:.1f}%)")
    print(f"   High-confidence accuracy: {high_conf_accuracy:.3f} ({high_conf_accuracy*100:.1f}%)")
    print(f"   High-confidence coverage: {high_conf_coverage:.3f} ({high_conf_coverage*100:.1f}%)")
    print(f"   High-confidence samples: {high_conf_correct}/{high_conf_total}")
    
    # Compare with expected results (98% high-conf, 72.9% coverage)
    print("\nüìà COMPARISON WITH VALIDATION SCRIPT:")
    print(f"   Expected high-conf accuracy: 98.0%")
    print(f"   Actual high-conf accuracy: {high_conf_accuracy*100:.1f}%")
    print(f"   Expected coverage: 72.9%")
    print(f"   Actual coverage: {high_conf_coverage*100:.1f}%")
    
    if high_conf_accuracy >= 0.95:
        print("‚úÖ Backend achieving expected high-confidence performance!")
    else:
        print("‚ö†Ô∏è  Backend high-confidence accuracy below expectations")
    
    if abs(high_conf_coverage - 0.729) < 0.1:
        print("‚úÖ Backend coverage matches validation script!")
    else:
        print("‚ö†Ô∏è  Backend coverage differs from validation script")
    
    # Save detailed results
    with open('backend_validation_results.json', 'w') as f:
        json.dump({
            'test_summary': {
                'total_tested': total_tested,
                'overall_accuracy': overall_accuracy,
                'high_conf_accuracy': high_conf_accuracy,
                'high_conf_coverage': high_conf_coverage,
                'high_conf_samples': f"{high_conf_correct}/{high_conf_total}"
            },
            'detailed_results': results
        }, f, indent=2)
    
    print(f"\nüíæ Detailed results saved to: backend_validation_results.json")

if __name__ == "__main__":
    main()
