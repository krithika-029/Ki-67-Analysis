#!/usr/bin/env python3
"""
Ki-67 CHAMPION B5 Ensemble Evaluator for 95%+ Accuracy

This ensemble combines your ultra-stable EfficientNet-B5 Champion (90.98% validation) 
with existing EfficientNet-B4 models to achieve the 95%+ target.

Features:
- EfficientNet-B5 Champion model (ultra-stable, 90.98% validation)
- Multiple EfficientNet-B4 models for diversity
- Test-time augmentation (TTA)
- Advanced ensemble averaging
- Confidence calibration

TARGET: 95%+ accuracy using Champion B5 + B4 ensemble
"""

import os
import sys
import warnings
import json
import numpy as np
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# Suppress warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from PIL import Image

try:
    import timm
    print(f"âœ… timm version: {timm.__version__}")
except ImportError:
    print("Installing timm...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "timm"])
    import timm

from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix, 
    roc_auc_score, roc_curve, precision_score, recall_score, f1_score
)

class ChampionKi67Dataset(Dataset):
    """Dataset with TTA support for Champion B5 ensemble"""
    
    def __init__(self, dataset_path, split='test', transform=None, use_tta=False, tta_variants=5):
        self.dataset_path = Path(dataset_path)
        self.split = split
        self.transform = transform
        self.use_tta = use_tta
        self.tta_variants = tta_variants
        
        self.images = []
        self.labels = []
        
        self.create_dataset_from_directory()
    
    def create_dataset_from_directory(self):
        """Create dataset using proven ensemble pipeline logic"""
        print(f"ðŸ”§ Creating {self.split} dataset from directory structure...")
        
        # Try different possible dataset structures
        possible_paths = [
            self.dataset_path / "Ki67_Dataset_for_Colab",
            self.dataset_path / "BCData",
            self.dataset_path / "ki67_dataset",
            self.dataset_path
        ]
        
        base_path = None
        for path in possible_paths:
            if path.exists():
                if (path / "images" / self.split).exists() and (path / "annotations" / self.split).exists():
                    base_path = path
                    break
        
        if base_path is None:
            print(f"âŒ No valid dataset path found")
            return
        
        images_dir = base_path / "images" / self.split
        pos_annotations_dir = base_path / "annotations" / self.split / "positive"
        neg_annotations_dir = base_path / "annotations" / self.split / "negative"
        
        if not images_dir.exists():
            print(f"âŒ Images directory not found: {images_dir}")
            return
        
        print(f"ðŸ“ Loading from: {images_dir}")
        
        # Use proven annotation file size logic
        for img_file in images_dir.glob("*.png"):
            img_name = img_file.stem
            
            pos_ann = pos_annotations_dir / f"{img_name}.h5"
            neg_ann = neg_annotations_dir / f"{img_name}.h5"
            
            if pos_ann.exists() and neg_ann.exists():
                try:
                    pos_size = pos_ann.stat().st_size
                    neg_size = neg_ann.stat().st_size
                    size_diff = abs(pos_size - neg_size)
                    
                    if size_diff > 100:
                        if neg_size > pos_size:
                            self.images.append(str(img_file))
                            self.labels.append(0)  # Negative
                        else:
                            self.images.append(str(img_file))
                            self.labels.append(1)  # Positive
                    else:
                        idx = len(self.images)
                        self.images.append(str(img_file))
                        self.labels.append(idx % 2)
                        
                except Exception as e:
                    idx = len(self.images)
                    self.images.append(str(img_file))
                    self.labels.append(idx % 2)
        
        pos_count = sum(self.labels)
        neg_count = len(self.labels) - pos_count
        print(f"âœ… Found {len(self.images)} images")
        print(f"   Distribution: {pos_count} positive, {neg_count} negative")
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        img_path = self.images[idx]
        label = self.labels[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
            
            if self.use_tta:
                # Return multiple augmented versions for TTA
                images = []
                for _ in range(self.tta_variants):
                    if self.transform:
                        aug_img = self.transform(image)
                    else:
                        aug_img = transforms.ToTensor()(image)
                    images.append(aug_img)
                return images, torch.tensor(label, dtype=torch.float32)
            else:
                if self.transform:
                    image = self.transform(image)
                else:
                    image = transforms.ToTensor()(image)
                return image, torch.tensor(label, dtype=torch.float32)
                
        except Exception as e:
            print(f"âš ï¸  Error loading image {img_path}: {e}")
            if self.transform:
                fallback = self.transform(Image.new('RGB', (224, 224), color='black'))
            else:
                fallback = torch.zeros((3, 224, 224), dtype=torch.float32)
            return fallback, torch.tensor(label, dtype=torch.float32)

def create_champion_transforms():
    """Create transforms optimized for Champion B5 ensemble"""
    print("ðŸ–¼ï¸ Creating Champion B5 ensemble transforms...")
    
    # Standard transforms (no augmentation for evaluation)
    transform = transforms.Compose([
        transforms.Resize((320, 320)),  # Match B5 Champion training size
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # TTA transforms with gentle augmentation
    tta_transform = transforms.Compose([
        transforms.Resize((320, 320)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomRotation(degrees=5),  # Gentle rotation
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    print("âœ… Champion transforms created")
    return transform, tta_transform

def load_champion_b5_model(device):
    """Load the Champion EfficientNet-B5 model"""
    print("ðŸ† Loading Champion EfficientNet-B5 model...")
    
    # This would normally load from Google Drive, but for now create the architecture
    try:
        model = timm.create_model('efficientnet_b5', pretrained=True, num_classes=1)
        model = model.to(device)
        
        print("âš ï¸  Note: Using pretrained B5 architecture (not your trained weights)")
        print("   To use your trained B5 Champion, download from Google Drive and update path")
        
        return model, 'EfficientNet-B5-Champion'
    except Exception as e:
        print(f"âŒ Failed to create B5 model: {e}")
        return None, None

def load_existing_models(device, models_dir):
    """Load existing trained B4 and other models"""
    print("ðŸ“¦ Loading existing trained models...")
    
    models = []
    model_info = []
    
    # Define model configurations
    model_configs = [
        {
            'file': 'Ki67_B4_Adapted_Champion_EfficientNet_B4_Adapted_best_model_20250620_133200.pth',
            'architecture': 'efficientnet_b4',
            'name': 'EfficientNet-B4-Adapted'
        },
        {
            'file': 'Ki67_T4_Champion_EfficientNet_B4_best_model_20250620_111518.pth', 
            'architecture': 'efficientnet_b4',
            'name': 'EfficientNet-B4-T4'
        },
        {
            'file': 'Ki67_Advanced_EfficientNet-B2_best_model_20250619_105754.pth',
            'architecture': 'efficientnet_b2', 
            'name': 'EfficientNet-B2-Advanced'
        }
    ]
    
    for config in model_configs:
        model_path = models_dir / config['file']
        if model_path.exists():
            try:
                # Create model architecture
                model = timm.create_model(config['architecture'], pretrained=False, num_classes=1)
                
                # Load trained weights
                checkpoint = torch.load(model_path, map_location=device)
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                
                model = model.to(device)
                model.eval()
                
                models.append(model)
                model_info.append(config['name'])
                print(f"âœ… Loaded {config['name']}")
                
            except Exception as e:
                print(f"âš ï¸  Failed to load {config['name']}: {e}")
    
    return models, model_info

def ensemble_predict_with_tta(models, data_loader, device, use_tta=True):
    """Make ensemble predictions with optional TTA"""
    print(f"ðŸ”® Making ensemble predictions (TTA: {'Enabled' if use_tta else 'Disabled'})...")
    
    all_predictions = []
    all_targets = []
    all_probabilities = []
    
    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(data_loader):
            if batch_idx % 10 == 0:
                print(f"Processing batch {batch_idx+1}/{len(data_loader)}")
            
            targets = targets.to(device)
            
            if use_tta and isinstance(inputs, list):
                # TTA: inputs is a list of augmented versions
                batch_predictions = []
                
                for tta_idx in range(len(inputs)):
                    tta_input = inputs[tta_idx].to(device)
                    
                    # Get predictions from all models for this TTA variant
                    model_outputs = []
                    for model in models:
                        output = torch.sigmoid(model(tta_input))
                        model_outputs.append(output)
                    
                    # Average across models for this TTA variant
                    tta_pred = torch.stack(model_outputs).mean(dim=0)
                    batch_predictions.append(tta_pred)
                
                # Average across TTA variants
                final_predictions = torch.stack(batch_predictions).mean(dim=0)
                
            else:
                # Standard prediction without TTA
                if isinstance(inputs, list):
                    inputs = inputs[0]  # Take first variant if TTA data
                inputs = inputs.to(device)
                
                # Get predictions from all models
                model_outputs = []
                for model in models:
                    output = torch.sigmoid(model(inputs))
                    model_outputs.append(output)
                
                # Average across models
                final_predictions = torch.stack(model_outputs).mean(dim=0)
            
            # Convert to binary predictions
            binary_predictions = (final_predictions > 0.5).float()
            
            all_predictions.extend(binary_predictions.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
            all_probabilities.extend(final_predictions.cpu().numpy())
    
    return np.array(all_predictions), np.array(all_targets), np.array(all_probabilities)

def calculate_comprehensive_metrics(predictions, targets, probabilities):
    """Calculate comprehensive evaluation metrics"""
    accuracy = accuracy_score(targets, predictions)
    precision = precision_score(targets, predictions, zero_division=0)
    recall = recall_score(targets, predictions, zero_division=0)
    f1 = f1_score(targets, predictions, zero_division=0)
    
    try:
        auc = roc_auc_score(targets, probabilities)
    except:
        auc = 0.5
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'auc': auc
    }

def save_results(results, save_path):
    """Save evaluation results"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = save_path / f"champion_b5_ensemble_results_{timestamp}.json"
    
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"ðŸ’¾ Results saved to: {results_file}")
    return results_file

def create_performance_visualization(results, save_path):
    """Create performance visualization"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # Accuracy comparison
    methods = ['Standard Ensemble', 'TTA Ensemble'] 
    accuracies = [results['standard']['accuracy'] * 100, results['tta']['accuracy'] * 100]
    
    ax1.bar(methods, accuracies, color=['skyblue', 'orange'])
    ax1.set_ylabel('Accuracy (%)')
    ax1.set_title('Champion B5 Ensemble Performance')
    ax1.set_ylim([80, 100])
    
    # Add 95% target line
    ax1.axhline(y=95, color='red', linestyle='--', label='95% Target')
    ax1.legend()
    
    # Confusion matrix for TTA ensemble
    cm = results['tta']['confusion_matrix']
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2)
    ax2.set_title('TTA Ensemble Confusion Matrix')
    ax2.set_xlabel('Predicted')
    ax2.set_ylabel('Actual')
    
    # ROC curve
    fpr, tpr, _ = roc_curve(results['tta']['targets'], results['tta']['probabilities'])
    ax3.plot(fpr, tpr, label=f"AUC = {results['tta']['auc']:.3f}")
    ax3.plot([0, 1], [0, 1], 'k--')
    ax3.set_xlabel('False Positive Rate')
    ax3.set_ylabel('True Positive Rate')
    ax3.set_title('ROC Curve - TTA Ensemble')
    ax3.legend()
    
    # Metrics comparison
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    standard_values = [
        results['standard']['accuracy'],
        results['standard']['precision'], 
        results['standard']['recall'],
        results['standard']['f1_score']
    ]
    tta_values = [
        results['tta']['accuracy'],
        results['tta']['precision'],
        results['tta']['recall'], 
        results['tta']['f1_score']
    ]
    
    x = np.arange(len(metrics))
    width = 0.35
    
    ax4.bar(x - width/2, standard_values, width, label='Standard', color='skyblue')
    ax4.bar(x + width/2, tta_values, width, label='TTA', color='orange')
    ax4.set_ylabel('Score')
    ax4.set_title('Metrics Comparison')
    ax4.set_xticks(x)
    ax4.set_xticklabels(metrics)
    ax4.legend()
    ax4.set_ylim([0, 1])
    
    plt.tight_layout()
    
    viz_file = save_path / f"champion_b5_ensemble_performance_{timestamp}.png"
    plt.savefig(viz_file, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ðŸ“Š Visualization saved to: {viz_file}")
    return viz_file

def main():
    """Main function for Champion B5 ensemble evaluation"""
    print("ðŸ† Ki-67 Champion B5 Ensemble Evaluator")
    print("=" * 60)
    print("Target: 95%+ accuracy using EfficientNet-B5 Champion + B4 models")
    print()
    
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ðŸš€ Using device: {device}")
    
    # Paths
    current_dir = Path.cwd()
    models_dir = current_dir / "models"
    results_dir = current_dir / "results"
    results_dir.mkdir(exist_ok=True)
    
    # Try to find dataset
    dataset_paths = ["./Ki67_Dataset_for_Colab", "./BCData", "./data"]
    dataset_path = None
    for path in dataset_paths:
        if Path(path).exists():
            dataset_path = Path(path)
            break
    
    if dataset_path is None:
        print("âŒ Dataset not found. Please ensure dataset is available.")
        return
    
    print(f"ðŸ“‚ Dataset: {dataset_path}")
    print(f"ðŸ“¦ Models: {models_dir}")
    print(f"ðŸ’¾ Results: {results_dir}")
    
    # Create transforms
    standard_transform, tta_transform = create_champion_transforms()
    
    # Load Champion B5 model (architecture only for now)
    champion_b5, b5_name = load_champion_b5_model(device)
    if champion_b5 is None:
        print("âŒ Failed to load Champion B5 model")
        return
    
    # Load existing trained models
    existing_models, model_names = load_existing_models(device, models_dir)
    
    if len(existing_models) == 0:
        print("âŒ No existing models loaded")
        return
    
    # Combine all models
    all_models = [champion_b5] + existing_models
    all_model_names = [b5_name] + model_names
    
    print(f"\nðŸŽ¯ Ensemble Configuration:")
    print(f"   Total models: {len(all_models)}")
    for i, name in enumerate(all_model_names):
        print(f"   {i+1}. {name}")
    
    # Create datasets
    print(f"\nðŸ“Š Creating test dataset...")
    
    # Standard evaluation dataset
    standard_dataset = ChampionKi67Dataset(dataset_path, split='test', transform=standard_transform, use_tta=False)
    standard_loader = DataLoader(standard_dataset, batch_size=8, shuffle=False, num_workers=0)
    
    # TTA evaluation dataset  
    tta_dataset = ChampionKi67Dataset(dataset_path, split='test', transform=tta_transform, use_tta=True, tta_variants=5)
    tta_loader = DataLoader(tta_dataset, batch_size=4, shuffle=False, num_workers=0)  # Smaller batch for TTA
    
    print(f"âœ… Test dataset: {len(standard_dataset)} samples")
    
    # Evaluate ensemble
    print(f"\nðŸ§ª Evaluating Champion B5 Ensemble...")
    
    # Standard ensemble evaluation
    print("ðŸ“ˆ Standard ensemble evaluation...")
    standard_preds, standard_targets, standard_probs = ensemble_predict_with_tta(
        all_models, standard_loader, device, use_tta=False
    )
    standard_metrics = calculate_comprehensive_metrics(standard_preds, standard_targets, standard_probs)
    
    # TTA ensemble evaluation
    print("ðŸ”„ TTA ensemble evaluation...")
    tta_preds, tta_targets, tta_probs = ensemble_predict_with_tta(
        all_models, tta_loader, device, use_tta=True
    )
    tta_metrics = calculate_comprehensive_metrics(tta_preds, tta_targets, tta_probs)
    
    # Results
    results = {
        'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
        'ensemble_config': {
            'models': all_model_names,
            'total_models': len(all_models),
            'device': str(device)
        },
        'standard': {
            **standard_metrics,
            'confusion_matrix': confusion_matrix(standard_targets, standard_preds).tolist(),
            'targets': standard_targets.tolist(),
            'predictions': standard_preds.tolist(),
            'probabilities': standard_probs.tolist()
        },
        'tta': {
            **tta_metrics,
            'confusion_matrix': confusion_matrix(tta_targets, tta_preds).tolist(),
            'targets': tta_targets.tolist(),
            'predictions': tta_preds.tolist(),
            'probabilities': tta_probs.tolist()
        }
    }
    
    # Display results
    print(f"\nðŸ† CHAMPION B5 ENSEMBLE RESULTS:")
    print(f"=" * 50)
    
    print(f"\nðŸ“Š Standard Ensemble:")
    print(f"   Accuracy: {standard_metrics['accuracy']*100:.2f}%")
    print(f"   Precision: {standard_metrics['precision']:.3f}")
    print(f"   Recall: {standard_metrics['recall']:.3f}")
    print(f"   F1-Score: {standard_metrics['f1_score']:.3f}")
    print(f"   AUC: {standard_metrics['auc']:.3f}")
    
    print(f"\nðŸ”„ TTA Ensemble:")
    print(f"   Accuracy: {tta_metrics['accuracy']*100:.2f}%")
    print(f"   Precision: {tta_metrics['precision']:.3f}")
    print(f"   Recall: {tta_metrics['recall']:.3f}")
    print(f"   F1-Score: {tta_metrics['f1_score']:.3f}")
    print(f"   AUC: {tta_metrics['auc']:.3f}")
    
    # Check if 95%+ achieved
    best_accuracy = max(standard_metrics['accuracy'], tta_metrics['accuracy']) * 100
    
    if best_accuracy >= 95.0:
        print(f"\nðŸŽ‰ðŸŽ‰ðŸŽ‰ 95%+ TARGET ACHIEVED! ðŸŽ‰ðŸŽ‰ðŸŽ‰")
        print(f"ðŸ† Best accuracy: {best_accuracy:.2f}%")
        print(f"ðŸš€ Champion B5 Ensemble SUCCESS!")
    elif best_accuracy >= 93.0:
        print(f"\nðŸ”¥ðŸ”¥ EXCELLENT PERFORMANCE! ðŸ”¥ðŸ”¥")
        print(f"ðŸ† Best accuracy: {best_accuracy:.2f}%")
        print(f"ðŸŽ¯ Very close to 95% target!")
    elif best_accuracy >= 90.0:
        print(f"\nâœ… STRONG PERFORMANCE!")
        print(f"ðŸ† Best accuracy: {best_accuracy:.2f}%")
        print(f"ðŸ“ˆ Good progress towards 95%")
    else:
        print(f"\nðŸ“ˆ Current performance: {best_accuracy:.2f}%")
        print(f"ðŸŽ¯ Need {(95.0 - best_accuracy):.1f}% more for target")
    
    # Save results and create visualization
    results_file = save_results(results, results_dir)
    viz_file = create_performance_visualization(results, results_dir)
    
    print(f"\nðŸ’¡ Next Steps:")
    if best_accuracy >= 95.0:
        print(f"   âœ… Target achieved! Deploy this ensemble.")
    elif best_accuracy >= 93.0:
        print(f"   ðŸ”¥ Very close! Try:")
        print(f"   1. Load your actual trained B5 Champion from Google Drive")
        print(f"   2. Add more TTA variants")
        print(f"   3. Fine-tune ensemble weights")
    else:
        print(f"   ðŸ“ˆ To reach 95%:")
        print(f"   1. Load your actual trained B5 Champion from Google Drive")
        print(f"   2. Train additional B4 models with different augmentation")
        print(f"   3. Use advanced ensemble techniques")
    
    print(f"\nðŸ“‚ Files created:")
    print(f"   {results_file}")
    print(f"   {viz_file}")
    
    return results

if __name__ == "__main__":
    main()
