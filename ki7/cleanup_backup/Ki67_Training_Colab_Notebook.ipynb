{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5b07f6",
   "metadata": {},
   "source": [
    "# Ki-67 Scoring System - Google Colab Training\n",
    "\n",
    "This notebook implements a comprehensive Ki-67 scoring system using ensemble deep learning models for breast cancer diagnosis.\n",
    "\n",
    "## Features:\n",
    "- **Three Model Ensemble**: InceptionV3, ResNet-50, and Vision Transformer\n",
    "- **Google Drive Integration**: Automatic dataset loading and model saving\n",
    "- **Robust Training**: Error handling and early stopping\n",
    "- **Comprehensive Evaluation**: Multiple metrics and visualizations\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload your dataset ZIP to: `MyDrive/Ki67_Dataset/Ki67_Dataset_for_Colab.zip`\n",
    "2. Run cells sequentially\n",
    "3. Models will be saved to your MyDrive root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Packages\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install package with error handling\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "        print(f\"‚úÖ {package}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"‚ö†Ô∏è  {package} - may already be installed\")\n",
    "        return False\n",
    "\n",
    "print(\"üì¶ Installing required packages...\")\n",
    "packages = [\"torch\", \"torchvision\", \"scikit-learn\", \"matplotlib\", \"seaborn\", \"pandas\", \"numpy\", \"Pillow\", \"timm\"]\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\n‚úÖ Package installation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d137e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import shutil\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "try:\n",
    "    import timm\n",
    "    print(\"‚úÖ timm imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  timm not available, will use fallback CNN\")\n",
    "    timm = None\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce700b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and Setup Paths\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Setup save paths\n",
    "MODELS_SAVE_PATH = \"/content/drive/MyDrive\"  # Models saved to MyDrive root\n",
    "RESULTS_SAVE_PATH = \"/content/drive/MyDrive/Ki67_Results\"\n",
    "os.makedirs(RESULTS_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Models will be saved to: {MODELS_SAVE_PATH}\")\n",
    "print(f\"üìÅ Results will be saved to: {RESULTS_SAVE_PATH}\")\n",
    "\n",
    "# Check what's in your Ki67_Dataset folder\n",
    "dataset_folder = \"/content/drive/MyDrive/Ki67_Dataset\"\n",
    "if os.path.exists(dataset_folder):\n",
    "    print(f\"\\nüìã Contents of Ki67_Dataset folder:\")\n",
    "    for item in os.listdir(dataset_folder):\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(f\"‚ùå Ki67_Dataset folder not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Dataset from Google Drive\n",
    "DATASET_ZIP_PATH = \"/content/drive/MyDrive/Ki67_Dataset/Ki67_Dataset_for_Colab.zip\"\n",
    "\n",
    "if os.path.exists(DATASET_ZIP_PATH):\n",
    "    print(f\"‚úÖ Found dataset at: {DATASET_ZIP_PATH}\")\n",
    "    \n",
    "    # Create extraction directory\n",
    "    EXTRACT_PATH = \"/content/ki67_dataset\"\n",
    "    os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
    "    \n",
    "    # Extract the dataset\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(DATASET_ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_PATH)\n",
    "    \n",
    "    print(\"‚úÖ Dataset extracted successfully!\")\n",
    "    \n",
    "    # List extracted contents\n",
    "    print(\"\\nExtracted contents:\")\n",
    "    for root, dirs, files in os.walk(EXTRACT_PATH):\n",
    "        level = root.replace(EXTRACT_PATH, '').count(os.sep)\n",
    "        if level < 3:  # Limit depth for readability\n",
    "            indent = ' ' * 2 * level\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            if level < 2:  # Show files only for top 2 levels\n",
    "                subindent = ' ' * 2 * (level + 1)\n",
    "                for file in files[:3]:  # Show first 3 files\n",
    "                    print(f\"{subindent}{file}\")\n",
    "                if len(files) > 3:\n",
    "                    print(f\"{subindent}... and {len(files)-3} more files\")\n",
    "    \n",
    "    DATASET_PATH = EXTRACT_PATH\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Dataset ZIP file not found!\")\n",
    "    print(\"Available files in Ki67_Dataset:\")\n",
    "    if os.path.exists(\"/content/drive/MyDrive/Ki67_Dataset\"):\n",
    "        for item in os.listdir(\"/content/drive/MyDrive/Ki67_Dataset\"):\n",
    "            print(f\"  - {item}\")\n",
    "    \n",
    "    # You can manually set the path here if ZIP file has different name\n",
    "    DATASET_PATH = \"/content/ki67_dataset\"  # Will be empty but won't crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Dataset Class\n",
    "class Ki67Dataset(Dataset):\n",
    "    def __init__(self, dataset_path, split='train', transform=None):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load data\n",
    "        self.load_from_directory()\n",
    "    \n",
    "    def load_from_directory(self):\n",
    "        \"\"\"Load dataset from directory structure\"\"\"\n",
    "        print(f\"Loading {self.split} data from directory structure...\")\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Check multiple possible directory structures\n",
    "        possible_structures = [\n",
    "            # Standard BCData structure\n",
    "            {\n",
    "                'images': self.dataset_path / \"BCData\" / \"images\" / self.split,\n",
    "                'pos_annotations': self.dataset_path / \"BCData\" / \"annotations\" / self.split / \"positive\",\n",
    "                'neg_annotations': self.dataset_path / \"BCData\" / \"annotations\" / self.split / \"negative\"\n",
    "            },\n",
    "            # Alternative structure\n",
    "            {\n",
    "                'images': self.dataset_path / \"images\" / self.split,\n",
    "                'pos_annotations': self.dataset_path / \"annotations\" / self.split / \"positive\",\n",
    "                'neg_annotations': self.dataset_path / \"annotations\" / self.split / \"negative\"\n",
    "            },\n",
    "            # Test256 structure (adapted for splits)\n",
    "            {\n",
    "                'images': self.dataset_path / \"data\" / \"test256\",\n",
    "                'json_annotations': True\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        data_found = False\n",
    "        for structure in possible_structures:\n",
    "            if 'json_annotations' in structure:\n",
    "                images_dir = structure['images']\n",
    "                if images_dir.exists():\n",
    "                    self._load_from_json_structure(images_dir)\n",
    "                    data_found = True\n",
    "                    break\n",
    "            else:\n",
    "                images_dir = structure['images']\n",
    "                pos_annotations_dir = structure['pos_annotations']\n",
    "                neg_annotations_dir = structure['neg_annotations']\n",
    "                \n",
    "                if images_dir.exists():\n",
    "                    self._load_from_h5_structure(images_dir, pos_annotations_dir, neg_annotations_dir)\n",
    "                    data_found = True\n",
    "                    break\n",
    "        \n",
    "        if not data_found:\n",
    "            print(f\"‚ö†Ô∏è  No data found for {self.split} split in any expected structure\")\n",
    "            # Show what's actually available\n",
    "            print(\"Available directories:\")\n",
    "            if self.dataset_path.exists():\n",
    "                for item in self.dataset_path.iterdir():\n",
    "                    if item.is_dir():\n",
    "                        print(f\"  - {item.name}/\")\n",
    "            \n",
    "            self.images = []\n",
    "            self.labels = []\n",
    "    \n",
    "    def _load_from_json_structure(self, images_dir):\n",
    "        \"\"\"Load from JSON annotation structure and create splits\"\"\"\n",
    "        all_images = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for img_file in images_dir.glob(\"*.jpg\"):\n",
    "            json_file = img_file.with_suffix('.json')\n",
    "            if json_file.exists():\n",
    "                try:\n",
    "                    with open(json_file, 'r') as f:\n",
    "                        annotation = json.load(f)\n",
    "                    \n",
    "                    # Determine label from JSON\n",
    "                    label = 0\n",
    "                    if 'shapes' in annotation and len(annotation['shapes']) > 0:\n",
    "                        label = 1\n",
    "                    elif 'label' in annotation:\n",
    "                        label = 1 if annotation['label'] == 'positive' else 0\n",
    "                    elif 'ki67_positive' in annotation:\n",
    "                        label = int(annotation['ki67_positive'])\n",
    "                    \n",
    "                    all_images.append(str(img_file))\n",
    "                    all_labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not load {json_file}: {e}\")\n",
    "        \n",
    "        # Create splits\n",
    "        if all_images:\n",
    "            indices = np.random.RandomState(42).permutation(len(all_images))\n",
    "            n_total = len(indices)\n",
    "            n_train = int(0.7 * n_total)\n",
    "            n_val = int(0.15 * n_total)\n",
    "            \n",
    "            if self.split == 'train':\n",
    "                selected_indices = indices[:n_train]\n",
    "            elif self.split == 'validation':\n",
    "                selected_indices = indices[n_train:n_train+n_val]\n",
    "            else:  # test\n",
    "                selected_indices = indices[n_train+n_val:]\n",
    "            \n",
    "            self.images = [all_images[i] for i in selected_indices]\n",
    "            self.labels = [all_labels[i] for i in selected_indices]\n",
    "    \n",
    "    def _load_from_h5_structure(self, images_dir, pos_annotations_dir, neg_annotations_dir):\n",
    "        \"\"\"Load from h5 annotation structure\"\"\"\n",
    "        for img_file in images_dir.glob(\"*.png\"):\n",
    "            img_name = img_file.stem\n",
    "            pos_ann = pos_annotations_dir / f\"{img_name}.h5\"\n",
    "            neg_ann = neg_annotations_dir / f\"{img_name}.h5\"\n",
    "\n",
    "            if pos_ann.exists():\n",
    "                self.images.append(str(img_file))\n",
    "                self.labels.append(1)\n",
    "            elif neg_ann.exists():\n",
    "                self.images.append(str(img_file))\n",
    "                self.labels.append(0)\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} samples from directory\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, torch.tensor(label, dtype=torch.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return fallback\n",
    "            if self.transform:\n",
    "                fallback = self.transform(Image.new('RGB', (640, 640), color='black'))\n",
    "            else:\n",
    "                fallback = torch.zeros((3, 224, 224))\n",
    "            return fallback, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "print(\"‚úÖ Dataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6094bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Transforms and Datasets\n",
    "# Define data transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "print(\"üîÑ Creating datasets...\")\n",
    "train_dataset = Ki67Dataset(DATASET_PATH, split='train', transform=train_transform)\n",
    "val_dataset = Ki67Dataset(DATASET_PATH, split='validation', transform=val_transform)\n",
    "test_dataset = Ki67Dataset(DATASET_PATH, split='test', transform=val_transform)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset creation completed!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Check class distribution\n",
    "def check_class_distribution(dataset, name):\n",
    "    if len(dataset) > 0:\n",
    "        labels = []\n",
    "        for i in range(min(len(dataset), 100)):\n",
    "            _, label = dataset[i]\n",
    "            labels.append(int(label.item()))\n",
    "        \n",
    "        pos_count = sum(labels)\n",
    "        neg_count = len(labels) - pos_count\n",
    "        print(f\"{name}: {pos_count} positive, {neg_count} negative (from {len(labels)} checked)\")\n",
    "\n",
    "if len(train_dataset) > 0:\n",
    "    check_class_distribution(train_dataset, \"Training\")\n",
    "    check_class_distribution(val_dataset, \"Validation\")\n",
    "    check_class_distribution(test_dataset, \"Test\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data loaded. Please check your dataset structure.\")\n",
    "    print(\"Expected structures:\")\n",
    "    print(\"1. BCData/images/{train,validation,test}/ with BCData/annotations/{train,validation,test}/{positive,negative}/\")\n",
    "    print(\"2. images/{train,validation,test}/ with annotations/{train,validation,test}/{positive,negative}/\")\n",
    "    print(\"3. data/test256/ with .jpg and .json files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2438f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Loaders (only if we have data)\n",
    "if len(train_dataset) > 0:\n",
    "    batch_size = 32 if torch.cuda.is_available() else 16\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"‚úÖ Data loaders created with batch size: {batch_size}\")\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Test loading a batch\n",
    "    try:\n",
    "        sample_batch = next(iter(train_loader))\n",
    "        images, labels = sample_batch\n",
    "        print(f\"Sample batch shape: images={images.shape}, labels={labels.shape}\")\n",
    "        print(\"‚úÖ Data loading test successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data loading test failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot create data loaders - no training data available\")\n",
    "    print(\"Please check your dataset and re-run the dataset creation cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67629c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation and Setup (only if we have data)\n",
    "if len(train_dataset) > 0:\n",
    "    print(\"üèóÔ∏è Creating models...\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    labels = []\n",
    "    for i in range(len(train_dataset)):\n",
    "        _, label = train_dataset[i]\n",
    "        labels.append(int(label.item()))\n",
    "    \n",
    "    pos_count = sum(labels)\n",
    "    neg_count = len(labels) - pos_count\n",
    "    \n",
    "    if pos_count > 0 and neg_count > 0:\n",
    "        pos_weight = len(labels) / (2 * pos_count)\n",
    "        neg_weight = len(labels) / (2 * neg_count)\n",
    "        pos_weight_ratio = pos_weight / neg_weight\n",
    "    else:\n",
    "        pos_weight_ratio = 1.0\n",
    "    \n",
    "    print(f\"Class distribution: {neg_count} negative, {pos_count} positive\")\n",
    "    print(f\"Positive weight ratio: {pos_weight_ratio:.3f}\")\n",
    "    \n",
    "    # Create models\n",
    "    models_dict = {}\n",
    "    \n",
    "    try:\n",
    "        # InceptionV3\n",
    "        inception_model = models.inception_v3(pretrained=True)\n",
    "        inception_model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(inception_model.fc.in_features, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        inception_model = inception_model.to(device)\n",
    "        \n",
    "        models_dict['inception'] = {\n",
    "            'model': inception_model,\n",
    "            'criterion': nn.BCELoss(),\n",
    "            'optimizer': optim.Adam(inception_model.parameters(), lr=0.001, weight_decay=1e-4),\n",
    "            'scheduler': ReduceLROnPlateau(optim.Adam(inception_model.parameters(), lr=0.001, weight_decay=1e-4), mode='min', factor=0.1, patience=5),\n",
    "            'name': 'InceptionV3'\n",
    "        }\n",
    "        print(\"‚úÖ InceptionV3 model created\")\n",
    "        \n",
    "        # ResNet-50\n",
    "        resnet_model = models.resnet50(pretrained=False)\n",
    "        resnet_model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(resnet_model.fc.in_features, 1)\n",
    "        )\n",
    "        resnet_model = resnet_model.to(device)\n",
    "        \n",
    "        models_dict['resnet'] = {\n",
    "            'model': resnet_model,\n",
    "            'criterion': nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight_ratio).to(device)),\n",
    "            'optimizer': optim.SGD(resnet_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4),\n",
    "            'scheduler': StepLR(optim.SGD(resnet_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4), step_size=10, gamma=0.1),\n",
    "            'name': 'ResNet50'\n",
    "        }\n",
    "        print(\"‚úÖ ResNet-50 model created\")\n",
    "        \n",
    "        # ViT or CNN fallback\n",
    "        try:\n",
    "            if timm is not None:\n",
    "                vit_model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=1)\n",
    "                vit_model = nn.Sequential(vit_model, nn.Sigmoid()).to(device)\n",
    "                print(\"‚úÖ ViT model created\")\n",
    "            else:\n",
    "                raise ImportError(\"timm not available\")\n",
    "        except:\n",
    "            # Fallback CNN\n",
    "            class SimpleCNN(nn.Module):\n",
    "                def __init__(self):\n",
    "                    super(SimpleCNN, self).__init__()\n",
    "                    self.features = nn.Sequential(\n",
    "                        nn.Conv2d(3, 32, 3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2),\n",
    "                        nn.Conv2d(32, 64, 3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2),\n",
    "                        nn.AdaptiveAvgPool2d(7)\n",
    "                    )\n",
    "                    self.classifier = nn.Sequential(\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(64 * 7 * 7, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.5),\n",
    "                        nn.Linear(128, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "                \n",
    "                def forward(self, x):\n",
    "                    x = self.features(x)\n",
    "                    x = self.classifier(x)\n",
    "                    return x\n",
    "            \n",
    "            vit_model = SimpleCNN().to(device)\n",
    "            print(\"‚úÖ Simple CNN created as ViT fallback\")\n",
    "        \n",
    "        models_dict['vit'] = {\n",
    "            'model': vit_model,\n",
    "            'criterion': nn.BCELoss(),\n",
    "            'optimizer': optim.Adam(vit_model.parameters(), lr=1e-3, weight_decay=1e-4),\n",
    "            'scheduler': ReduceLROnPlateau(optim.Adam(vit_model.parameters(), lr=1e-3, weight_decay=1e-4), mode='min', factor=0.1, patience=5),\n",
    "            'name': 'ViT'\n",
    "        }\n",
    "        \n",
    "        # Fix optimizer references in schedulers\n",
    "        for key, model_info in models_dict.items():\n",
    "            if 'ReduceLR' in str(type(model_info['scheduler'])):\n",
    "                model_info['scheduler'] = ReduceLROnPlateau(model_info['optimizer'], mode='min', factor=0.1, patience=5)\n",
    "            elif 'StepLR' in str(type(model_info['scheduler'])):\n",
    "                model_info['scheduler'] = StepLR(model_info['optimizer'], step_size=10, gamma=0.1)\n",
    "        \n",
    "        print(f\"\\n‚úÖ All models created successfully!\")\n",
    "        \n",
    "        # Print model info\n",
    "        def count_parameters(model):\n",
    "            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\nüìä Model Parameters:\")\n",
    "        for key, model_info in models_dict.items():\n",
    "            count = count_parameters(model_info['model'])\n",
    "            print(f\"  {model_info['name']}: {count:,}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating models: {e}\")\n",
    "        models_dict = {}\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping model creation - no training data available\")\n",
    "    models_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving Functions\n",
    "def save_model_to_drive(model, model_name, epoch, val_loss, val_acc, save_path):\n",
    "    \"\"\"Save model checkpoint to Google Drive\"\"\"\n",
    "    try:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"Ki67_{model_name}_best_model_{timestamp}.pth\"\n",
    "        full_path = os.path.join(save_path, filename)\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'timestamp': timestamp,\n",
    "            'model_name': model_name,\n",
    "            'performance_summary': f\"Epoch {epoch}, Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\"\n",
    "        }, full_path)\n",
    "        \n",
    "        print(f\"‚úÖ Model saved to MyDrive: {filename}\")\n",
    "        return full_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save model {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_training_history(history, model_name, save_path):\n",
    "    \"\"\"Save training history\"\"\"\n",
    "    try:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{model_name}_history_{timestamp}.pkl\"\n",
    "        full_path = os.path.join(save_path, filename)\n",
    "        \n",
    "        with open(full_path, 'wb') as f:\n",
    "            pickle.dump(history, f)\n",
    "        \n",
    "        print(f\"‚úÖ Training history saved: {filename}\")\n",
    "        return full_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save history: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Model saving functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_individual_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "                          model_name, device, num_epochs=15, use_aux_loss=False, \n",
    "                          early_stopping_patience=7):\n",
    "    \"\"\"Train individual model with error handling\"\"\"\n",
    "    print(f\"\\nüöÄ Training {model_name}...\")\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} - {model_name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            try:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Fix label format\n",
    "                if labels.dim() == 1:\n",
    "                    labels = labels.unsqueeze(1)\n",
    "                labels = labels.float()\n",
    "                labels = torch.clamp(labels, 0.0, 1.0)\n",
    "                \n",
    "                # Adjust input size for InceptionV3\n",
    "                if model_name == \"InceptionV3\" and inputs.size(-1) != 299:\n",
    "                    inputs = F.interpolate(inputs, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                if use_aux_loss and model.training:\n",
    "                    outputs, aux_outputs = model(inputs)\n",
    "                    if outputs.dim() == 1:\n",
    "                        outputs = outputs.unsqueeze(1)\n",
    "                    if aux_outputs.dim() == 1:\n",
    "                        aux_outputs = aux_outputs.unsqueeze(1)\n",
    "                    \n",
    "                    outputs = torch.clamp(outputs, 1e-7, 1 - 1e-7)\n",
    "                    aux_outputs = torch.clamp(aux_outputs, 1e-7, 1 - 1e-7)\n",
    "                    \n",
    "                    main_loss = criterion(outputs, labels)\n",
    "                    aux_loss = criterion(aux_outputs, labels)\n",
    "                    loss = main_loss + 0.4 * aux_loss\n",
    "                else:\n",
    "                    outputs = model(inputs)\n",
    "                    if outputs.dim() == 1:\n",
    "                        outputs = outputs.unsqueeze(1)\n",
    "                    \n",
    "                    if isinstance(criterion, nn.BCELoss):\n",
    "                        outputs = torch.clamp(outputs, 1e-7, 1 - 1e-7)\n",
    "                    \n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    continue\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                if isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                else:\n",
    "                    predicted = (outputs > 0.5).float()\n",
    "                \n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                optimizer.zero_grad()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                continue\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                try:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    if labels.dim() == 1:\n",
    "                        labels = labels.unsqueeze(1)\n",
    "                    labels = labels.float()\n",
    "                    labels = torch.clamp(labels, 0.0, 1.0)\n",
    "                    \n",
    "                    if model_name == \"InceptionV3\" and inputs.size(-1) != 299:\n",
    "                        inputs = F.interpolate(inputs, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    \n",
    "                    if outputs.dim() == 1:\n",
    "                        outputs = outputs.unsqueeze(1)\n",
    "                    \n",
    "                    if isinstance(criterion, nn.BCELoss):\n",
    "                        outputs = torch.clamp(outputs, 1e-7, 1 - 1e-7)\n",
    "                    \n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    if isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "                        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                    else:\n",
    "                        predicted = (outputs > 0.5).float()\n",
    "                    \n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        # Calculate averages\n",
    "        train_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "        val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        train_acc = 100 * train_correct / train_total if train_total > 0 else 0\n",
    "        val_acc = 100 * val_correct / val_total if val_total > 0 else 0\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(\"‚úÖ New best model found!\")\n",
    "            \n",
    "            # Save to Google Drive\n",
    "            model_path = save_model_to_drive(model, model_name, epoch+1, val_loss, val_acc, MODELS_SAVE_PATH)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Step scheduler\n",
    "        if hasattr(scheduler, 'step'):\n",
    "            if 'ReduceLR' in str(type(scheduler)):\n",
    "                scheduler.step(val_loss)\n",
    "            elif 'Cyclic' not in str(type(scheduler)):\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"‚úÖ Best {model_name} model loaded!\")\n",
    "    \n",
    "    # Save training history\n",
    "    save_training_history(history, model_name, RESULTS_SAVE_PATH)\n",
    "    \n",
    "    return history, best_val_loss\n",
    "\n",
    "print(\"‚úÖ Training function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b3921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Training (only if we have data and models)\n",
    "if len(train_dataset) > 0 and len(models_dict) > 0:\n",
    "    print(\"üöÄ Starting training process...\")\n",
    "    \n",
    "    NUM_EPOCHS = 10  # Reduced for faster testing\n",
    "    individual_histories = {}\n",
    "    individual_best_losses = {}\n",
    "    \n",
    "    session_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(f\"üïê Training session: {session_timestamp}\")\n",
    "    \n",
    "    # Train each model\n",
    "    for key, model_info in models_dict.items():\n",
    "        try:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üèóÔ∏è TRAINING {model_info['name'].upper()} MODEL\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            use_aux_loss = (model_info['name'] == 'InceptionV3')\n",
    "            \n",
    "            history, best_loss = train_individual_model(\n",
    "                model_info['model'], train_loader, val_loader,\n",
    "                model_info['criterion'], model_info['optimizer'], model_info['scheduler'],\n",
    "                model_info['name'], device, NUM_EPOCHS, use_aux_loss=use_aux_loss\n",
    "            )\n",
    "            \n",
    "            individual_histories[model_info['name']] = history\n",
    "            individual_best_losses[model_info['name']] = best_loss\n",
    "            \n",
    "            print(f\"‚úÖ {model_info['name']} training completed\")\n",
    "            \n",
    "            # Clear CUDA cache\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {model_info['name']} training failed: {e}\")\n",
    "            individual_histories[model_info['name']] = {\n",
    "                'train_loss': [1.0], 'val_loss': [1.0],\n",
    "                'train_acc': [50.0], 'val_acc': [50.0]\n",
    "            }\n",
    "            individual_best_losses[model_info['name']] = 1.0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"‚úÖ TRAINING PROCESS COMPLETED!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nüìä Training Summary:\")\n",
    "    for model_name, best_loss in individual_best_losses.items():\n",
    "        final_val_acc = max(individual_histories[model_name]['val_acc'])\n",
    "        print(f\"  {model_name}: Best Loss={best_loss:.4f}, Best Acc={final_val_acc:.2f}%\")\n",
    "    \n",
    "    # Calculate ensemble weights\n",
    "    total_acc = sum(max(hist['val_acc']) for hist in individual_histories.values())\n",
    "    if total_acc > 0:\n",
    "        ensemble_weights = []\n",
    "        for model_name in individual_histories.keys():\n",
    "            val_acc = max(individual_histories[model_name]['val_acc'])\n",
    "            weight = val_acc / total_acc\n",
    "            ensemble_weights.append(weight)\n",
    "        \n",
    "        print(f\"\\n‚öñÔ∏è Calculated Ensemble Weights:\")\n",
    "        for i, (model_name, weight) in enumerate(zip(individual_histories.keys(), ensemble_weights)):\n",
    "            print(f\"  {model_name}: {weight:.4f}\")\n",
    "    else:\n",
    "        ensemble_weights = [1/3, 1/3, 1/3]\n",
    "        print(f\"\\n‚öñÔ∏è Using equal ensemble weights (fallback)\")\n",
    "    \n",
    "    # Save ensemble weights\n",
    "    try:\n",
    "        ensemble_path = os.path.join(MODELS_SAVE_PATH, f\"Ki67_ensemble_weights_{session_timestamp}.json\")\n",
    "        with open(ensemble_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'weights': ensemble_weights,\n",
    "                'model_order': list(individual_histories.keys()),\n",
    "                'session_timestamp': session_timestamp,\n",
    "                'best_losses': individual_best_losses,\n",
    "                'description': 'Ensemble weights for Ki67 classification'\n",
    "            }, f, indent=2)\n",
    "        print(f\"‚úÖ Ensemble weights saved to MyDrive: Ki67_ensemble_weights_{session_timestamp}.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not save ensemble weights: {e}\")\n",
    "    \n",
    "    print(\"\\nüéâ Training completed! Check your MyDrive for saved models.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot start training - missing data or models\")\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"  - No training data available\")\n",
    "    if len(models_dict) == 0:\n",
    "        print(\"  - Models not created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24145e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_models_and_save(models_dict, test_loader, device, ensemble_weights):\n",
    "    \"\"\"Evaluate models and save results\"\"\"\n",
    "    if len(test_loader.dataset) == 0:\n",
    "        print(\"‚ùå No test data available for evaluation\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"üîç Evaluating models on test set...\")\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    for model_info in models_dict.values():\n",
    "        model_info['model'].eval()\n",
    "    \n",
    "    predictions = {}\n",
    "    model_names = list(models_dict.keys())\n",
    "    for key in model_names:\n",
    "        predictions[models_dict[key]['name']] = []\n",
    "    predictions['Ensemble'] = []\n",
    "    \n",
    "    y_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            try:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                if labels.dim() == 1:\n",
    "                    labels = labels.unsqueeze(1)\n",
    "                labels = labels.float()\n",
    "                \n",
    "                model_outputs = {}\n",
    "                \n",
    "                # Get predictions from each model\n",
    "                for key, model_info in models_dict.items():\n",
    "                    try:\n",
    "                        model_inputs = inputs\n",
    "                        # Adjust for InceptionV3\n",
    "                        if model_info['name'] == \"InceptionV3\" and inputs.size(-1) != 299:\n",
    "                            model_inputs = F.interpolate(inputs, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "                        \n",
    "                        outputs = model_info['model'](model_inputs)\n",
    "                        if isinstance(outputs, tuple):\n",
    "                            outputs = outputs[0]\n",
    "                        if outputs.dim() == 1:\n",
    "                            outputs = outputs.unsqueeze(1)\n",
    "                        \n",
    "                        # Apply sigmoid if needed\n",
    "                        if isinstance(model_info['criterion'], nn.BCEWithLogitsLoss):\n",
    "                            outputs = torch.sigmoid(outputs)\n",
    "                        \n",
    "                        model_outputs[model_info['name']] = outputs\n",
    "                        predictions[model_info['name']].extend(outputs.cpu().numpy())\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        predictions[model_info['name']].extend([[0.5]] * len(labels))\n",
    "                        model_outputs[model_info['name']] = torch.ones_like(labels) * 0.5\n",
    "                \n",
    "                # Ensemble prediction\n",
    "                try:\n",
    "                    model_names_ordered = ['InceptionV3', 'ResNet50', 'ViT']\n",
    "                    ensemble_pred = torch.zeros_like(labels)\n",
    "                    for i, name in enumerate(model_names_ordered):\n",
    "                        if name in model_outputs:\n",
    "                            ensemble_pred += ensemble_weights[i] * model_outputs[name]\n",
    "                    \n",
    "                    predictions['Ensemble'].extend(ensemble_pred.cpu().numpy())\n",
    "                except:\n",
    "                    # Fallback to average\n",
    "                    avg_pred = torch.mean(torch.stack(list(model_outputs.values())), dim=0)\n",
    "                    predictions['Ensemble'].extend(avg_pred.cpu().numpy())\n",
    "                \n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    y_true = np.array(y_true).reshape(-1)\n",
    "    \n",
    "    print(f\"\\nüìä Evaluation Results:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, preds in predictions.items():\n",
    "        if len(preds) > 0:\n",
    "            scores = np.array(preds).reshape(-1)\n",
    "            pred_binary = (scores > 0.5).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = (pred_binary == y_true).mean() * 100\n",
    "            \n",
    "            try:\n",
    "                if len(np.unique(y_true)) > 1:\n",
    "                    auc = roc_auc_score(y_true, scores) * 100\n",
    "                else:\n",
    "                    auc = 50.0\n",
    "            except:\n",
    "                auc = 50.0\n",
    "            \n",
    "            try:\n",
    "                precision = precision_score(y_true, pred_binary, zero_division=0) * 100\n",
    "                recall = recall_score(y_true, pred_binary, zero_division=0) * 100\n",
    "                f1 = f1_score(y_true, pred_binary, zero_division=0) * 100\n",
    "            except:\n",
    "                precision = recall = f1 = 0.0\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1\n",
    "            }\n",
    "            \n",
    "            print(f\"{model_name:12}: Acc={accuracy:6.2f}%, AUC={auc:6.2f}%, F1={f1:6.2f}%\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Find best model\n",
    "    if results:\n",
    "        best_model = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
    "        print(f\"üèÜ Best model: {best_model} (Accuracy: {results[best_model]['accuracy']:.2f}%)\")\n",
    "    \n",
    "    # Save results\n",
    "    try:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results_file = os.path.join(RESULTS_SAVE_PATH, f\"Ki67_Results_Summary_{timestamp}.json\")\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'timestamp': timestamp,\n",
    "                'results': results,\n",
    "                'ensemble_weights': ensemble_weights,\n",
    "                'test_set_size': len(y_true)\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Results saved: Ki67_Results_Summary_{timestamp}.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not save results: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluation (only if we have trained models and test data)\n",
    "if len(train_dataset) > 0 and len(models_dict) > 0 and len(test_dataset) > 0 and 'ensemble_weights' in locals():\n",
    "    try:\n",
    "        results = evaluate_models_and_save(models_dict, test_loader, device, ensemble_weights)\n",
    "        \n",
    "        print(f\"\\nüéØ Evaluation completed!\")\n",
    "        print(f\"üìÅ All results saved to Google Drive\")\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"\\nüìä Final Summary:\")\n",
    "        print(f\"  Models saved to: MyDrive/\")\n",
    "        print(f\"  Results saved to: MyDrive/Ki67_Results/\")\n",
    "        if results:\n",
    "            best_model = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
    "            print(f\"  Best model: {best_model} ({results[best_model]['accuracy']:.2f}% accuracy)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Evaluation failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping evaluation - missing requirements:\")\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"  - No training data\")\n",
    "    if len(models_dict) == 0:\n",
    "        print(\"  - No models created\")\n",
    "    if len(test_dataset) == 0:\n",
    "        print(\"  - No test data\")\n",
    "    if 'ensemble_weights' not in locals():\n",
    "        print(\"  - No ensemble weights from training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e21c93",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### Files Saved to Your Google Drive:\n",
    "\n",
    "**MyDrive/** (Model files):\n",
    "- `Ki67_InceptionV3_best_model_YYYYMMDD_HHMMSS.pth`\n",
    "- `Ki67_ResNet50_best_model_YYYYMMDD_HHMMSS.pth` \n",
    "- `Ki67_ViT_best_model_YYYYMMDD_HHMMSS.pth`\n",
    "- `Ki67_ensemble_weights_YYYYMMDD_HHMMSS.json`\n",
    "\n",
    "**MyDrive/Ki67_Results/** (Analysis files):\n",
    "- Training histories (`.pkl` files)\n",
    "- Results summaries (`.json` files)\n",
    "- Detailed predictions and metrics\n",
    "\n",
    "### Loading Saved Models:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Load a saved model\n",
    "checkpoint = torch.load('/content/drive/MyDrive/Ki67_InceptionV3_best_model_YYYYMMDD_HHMMSS.pth')\n",
    "print(f\"Model performance: {checkpoint['performance_summary']}\")\n",
    "\n",
    "# Load ensemble weights\n",
    "import json\n",
    "with open('/content/drive/MyDrive/Ki67_ensemble_weights_YYYYMMDD_HHMMSS.json', 'r') as f:\n",
    "    ensemble_config = json.load(f)\n",
    "    weights = ensemble_config['weights']\n",
    "    model_order = ensemble_config['model_order']\n",
    "```\n",
    "\n",
    "### Troubleshooting:\n",
    "- If no data loads: Check your ZIP file structure\n",
    "- If training fails: Verify sufficient GPU memory\n",
    "- If models don't save: Ensure Google Drive has space\n",
    "\n",
    "The ensemble approach provides robust Ki-67 classification by combining three different architectures!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
